{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enron Submission Free-Response Questions\n",
    "1. Summarize for us the goal of this project and how machine learning is useful in trying to accomplish it. As part of your answer, give some background on the dataset and how it can be used to answer the project question. Were there any outliers in the data when you got it, and how did you handle those? [relevant rubric items: “data exploration”, “outlier investigation”]\n",
    "\n",
    "The goal of this project is to successfully identify persons of interest within the Enron Corpus, which is the largest openly available dataset of email conversations available today.\n",
    "\n",
    "Machine learning is keenly appropriate for this type of project because:\n",
    "\n",
    "- The dataset is massive and has many possible dimensions, processing this data and gathering useful information or trends would be nearly impossible for a person to do, even with traditional programming methods\n",
    "- The dataset has so many potential questions to answer. It can be beneficial to set up a machine learning algorithm to identify points of interest in the dataset prior to digging deeper with a more focused study\n",
    "- Machine learning allows you to pivot relatively quickly with new information by tweaking the parameters of your algorithm instead of completely refactoring logic in existing code\n",
    "\n",
    "This dataset has has 3289 data points. There are 146 executives and only 21 are initially identified as persons of interest. I decided to use 8 features total for my classifier in an attempt to minimize (but not entirely eliminate) missing values. Some of the features I selected do have missing values.\n",
    "\n",
    "While doing initial data analysis I was able to identify some odd outliers, but due to feature selection I eliminated them entirely from consideration in my machine learning algorith.\n",
    "\n",
    "- TOTAL seemed to potentially refer to something other than the individual (maybe the department the executive belonged to?)\n",
    "- Several 'executives' data points had only NULL/NaN values, I excluded all of these from consideration by popping them from the dictionary.\n",
    "\n",
    "2. What features did you end up using in your POI identifier, and what selection process did you use to pick them? Did you have to do any scaling? Why or why not? As part of the assignment, you should attempt to engineer your own feature that does not come ready-made in the dataset -- explain what feature you tried to make, and the rationale behind it. (You do not necessarily have to use it in the final analysis, only engineer and test it.) In your feature selection step, if you used an algorithm like a decision tree, please also give the feature importances of the features that you use, and if you used an automated feature selection function like SelectKBest, please report the feature scores and reasons for your choice of parameter values. [relevant rubric items: “create new features”, “properly scale features”, “intelligently select feature”]\n",
    "\n",
    "I loaded up the dataset into a Pandas dataframe to get a better look at the features available to use. I did a bit of exploration and selected features based on the following criteria:\n",
    "\n",
    "- Is there enough data available? (majority of the data is not NULL)\n",
    "- Did the feature (in my mind) have any correlation with the analysis I was trying to perform?\n",
    "\n",
    "Using this as a guide I selected several new features to include alongside the initial features in the starter code. The features I selected had specific reasons for being chosen, which I outline next to them. I wanted to focus very intently on conversations between known and potential persons of interest and any trend that could be found in their bonus and expenses in my attempts to identify more persons of interest.\n",
    "\n",
    "- bonus: If they received a higher-than-average bonus it's possible they were involved in the fraud\n",
    "- expenses: higher utilization of expenses could point to another person involved in fraud\n",
    "- from_poi_to_this_person: frequent conversations directly with confirmed poi's could indicate that the person is a poi themselves\n",
    "- from_this_person_to_poi: similar to above\n",
    "- shared_receipt_with_poi: similar to above\n",
    "\n",
    "After testing these features in step 3 I realized that I had cut the feature list down too far to get good precision out of the algorithms. I slowly began adding a single feature at a time and testing the algorithm. Some features drastically reduced the precision and recall (restricted_stock) while others incrementally improved it. Once I had all the features added back in I did a test on the tree features to see which features had the most impact on the decision tree:\n",
    "\n",
    "Tree Feature Importances:\n",
    "\n",
    "bonus : 0.2611\n",
    "long_term_incentive : 0.2129\n",
    "deferred_income : 0.1373\n",
    "exercised_stock_options : 0.1175\n",
    "expenses : 0.1051\n",
    "deferral_payments : 0.0764\n",
    "total_payments : 0.0594\n",
    "shared_receipt_with_poi : 0.0304\n",
    "salary : 0.0000\n",
    "from_poi_to_this_person : 0.0000\n",
    "from_this_person_to_poi : 0.0000\n",
    "loan_advances : 0.0000\n",
    "restricted_stock_deferred : 0.0000\n",
    "loan_advances : 0.0000\n",
    "director_fees : 0.0000\n",
    "total_stock_value : 0.0000\n",
    "custom_ratio_to_poi : 0.0000\n",
    "custom_ratio_from_poi : 0.0000\n",
    "\n",
    "Using this information I cut down the features list again and ran another test on the Decision Tree, which pushed the precision up past the necessary threshold.\n",
    "\n",
    "- Accuracy: 0.82733\t\n",
    "- Precision: 0.35804\t\n",
    "- Recall: 0.37200\n",
    "\n",
    "3. What algorithm did you end up using? What other one(s) did you try? How did model performance differ between algorithms? [relevant rubric item: “pick an algorithm”] What does it mean to tune the parameters of an algorithm, and what can happen if you don’t do this well? How did you tune the parameters of your particular algorithm? (Some algorithms do not have parameters that you need to tune -- if this is the case for the one you picked, identify and briefly explain how you would have done it for the model that was not your final choice or a different model that does utilize parameter tuning, e.g. a decision tree classifier). [relevant rubric item: “tune the algorithm”] What is validation, and what’s a classic mistake you can make if you do it wrong? How did you validate your analysis? [relevant rubric item: “validation strategy”]\n",
    "\n",
    "I initially chose a Decision Tree as my first algorithm. I initially tried adjusting the min_samples_split variable, setting it to higher-than-default settings in an attempt to normalize and prevent any sort of overfitting. After a few test runs I incorporated changes to max_depth, then random_state to see if I could push the precision measures higher until I got up to ~.25. When I couldn't squeeze any better performance out of the algorithm I tried it with the default settings and found that those settings yielded a better result. I am glad that I chose to use the default settings after attempting to manually change the parameters, it showed me how poorly clicking through parameters could have negative effects on your final accuracy measures. I believe that my overzealous cutting of features and focusing so closely in on two parts of the data that I assumed would be good for classification may have eliminated helpful features from the data, so I decided to return to feature selection after testing a second algorithm.\n",
    "\n",
    "Prior to going back to feature selection the best Decision Tree test I could get is the following:\n",
    "\n",
    "- Accuracy: 0.77862\t\n",
    "- Precision: 0.28995\t\n",
    "- Recall: 0.30300\n",
    "\n",
    "The second algorithm I tried was KMeans clustering, because I knew the number of categories I would be trying to classify each person into from the start (2). Since I knew the number of groups it made tuning the algorithm easy by setting the number of clusters to 2, but if this were improperly configured or if I tried to play with that parameter it would likely have severely detrimental effects on the precision of the algorithm, or it would misclassify things that should be classified differently. I did check how changing the cluster sized affected the algorithm, just to see what types of behavior occurred, but it did not give me any significant insights into the data.\n",
    "\n",
    "Prior to going back to feature selection the best KMeans test that I could get is the following:\n",
    "\n",
    "- Accuracy: 0.73054\t\n",
    "- Precision: 0.21480\t\n",
    "- Recall: 0.28300\n",
    "\n",
    "After removing two features that were showed to be unimportant (to_messages and from_messages) I ran the Decision Tree algorithm again. It showed a very slight improvement in precision but accuracy and recall dropped.\n",
    "\n",
    "- Accuracy: 0.76500\t\n",
    "- Precision: 0.29124\t\n",
    "- Recall: 0.28600\n",
    "\n",
    "Going back to feature selection and going through the process of adding features back in slowly, I finally came to a feature list that yielded a precision and recall score above the necessary threshold, but just barely.\n",
    "\n",
    "- Accuracy: 0.82733\t\n",
    "- Precision: 0.35804\t\n",
    "- Recall: 0.37200\n",
    "\n",
    "Those same features seemed to have a negative effect on the KMeans algorithm, however.\n",
    "\n",
    "- Accuracy: 0.81427\t\n",
    "- Precision: 0.20227\t\n",
    "- Recall: 0.13350\n",
    "\n",
    "4. Give at least 2 evaluation metrics and your average performance for each of them. Explain an interpretation of your metrics that says something human-understandable about your algorithm’s performance.\n",
    "\n",
    "For my final chosen algorithm (Decision Tree) I was able to get an average performance of 0.35804 Precision and 0.37200 Recall.\n",
    "\n",
    "Precision is the correctly predicted positive classifications against the number of actual positive classifications.\n",
    "\n",
    "Recall is the correctly predicted positive classifications against the number of positive classifications that **should** have been identified correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "sys.path.append(\"../tools/\")\n",
    "sys.path.append(\"C:\\Users\\Evernite.Evernite-NPC\\Desktop\\WGU C753/tools/\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data\n",
    "\n",
    "import tester\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import tree\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "### Task 1: Select what features you'll use.\n",
    "### features_list is a list of strings, each of which is a feature name.\n",
    "### The first feature must be \"poi\".\n",
    "features_list = ['poi','bonus','long_term_incentive','deferred_income','exercised_stock_options','expenses',\n",
    "                 'deferral_payments','total_payments','shared_receipt_with_poi'] # You will need to use more features\n",
    "\n",
    "### Load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "\n",
    "# Task 2: Remove outliers\n",
    "# Found these empty rows during exploration of the data and removed them\n",
    "data_dict.pop('LOCKHART EUGENE E', 0)\n",
    "data_dict.pop('TOTAL', 0)\n",
    "data_dict.pop('THE TRAVEL AGENCY IN THE PARK', 0)\n",
    "\n",
    "# Task 3: Create new feature(s)\n",
    "for val in data_dict.values():\n",
    "    from_this_person_to_poi = val['from_this_person_to_poi']\n",
    "    to_messages = val['to_messages']\n",
    "    from_poi_to_this_person = val['from_poi_to_this_person']\n",
    "    from_messages = val['from_messages']\n",
    "    \n",
    "    val[\"custom_ratio_to_poi\"] = (float(from_this_person_to_poi) / float(to_messages) \n",
    "                           if to_messages not in [0, \"NaN\"] and \n",
    "                              from_this_person_to_poi not in [0, \"NaN\"] \n",
    "                           else 0.0)\n",
    "    val[\"custom_ratio_from_poi\"] = (float(from_poi_to_this_person) / float(from_messages) \n",
    "                           if from_messages not in [0, \"NaN\"] and \n",
    "                              from_poi_to_this_person not in [0, \"NaN\"] \n",
    "                           else 0.0)\n",
    "\n",
    "# Append the new features to the feature list\n",
    "features_list.append('custom_ratio_to_poi')\n",
    "features_list.append('custom_ratio_from_poi')\n",
    "\n",
    "### Store to my_dataset for easy export below.\n",
    "my_dataset = data_dict\n",
    "\n",
    "### Extract features and labels from dataset for local testing\n",
    "data = featureFormat(my_dataset, features_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "# Split into training and testing sets for fitting and scoring\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(features, labels, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "poi = pd.DataFrame.from_dict(data_dict, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 143 entries, ALLEN PHILLIP K to YEAP SOON\n",
      "Data columns (total 23 columns):\n",
      "to_messages                  143 non-null object\n",
      "deferral_payments            143 non-null object\n",
      "custom_ratio_to_poi          143 non-null float64\n",
      "expenses                     143 non-null object\n",
      "poi                          143 non-null bool\n",
      "deferred_income              143 non-null object\n",
      "email_address                143 non-null object\n",
      "long_term_incentive          143 non-null object\n",
      "restricted_stock_deferred    143 non-null object\n",
      "shared_receipt_with_poi      143 non-null object\n",
      "loan_advances                143 non-null object\n",
      "from_messages                143 non-null object\n",
      "other                        143 non-null object\n",
      "custom_ratio_from_poi        143 non-null float64\n",
      "director_fees                143 non-null object\n",
      "bonus                        143 non-null object\n",
      "total_stock_value            143 non-null object\n",
      "from_poi_to_this_person      143 non-null object\n",
      "from_this_person_to_poi      143 non-null object\n",
      "restricted_stock             143 non-null object\n",
      "salary                       143 non-null object\n",
      "total_payments               143 non-null object\n",
      "exercised_stock_options      143 non-null object\n",
      "dtypes: bool(1), float64(2), object(20)\n",
      "memory usage: 25.8+ KB\n"
     ]
    }
   ],
   "source": [
    "poi.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Task 4: Try a varity of classifiers\n",
    "### Please name your classifier clf for easy export below.\n",
    "### Note that if you want to do PCA or other multi-stage operations,\n",
    "### you'll need to use Pipelines. For more info:\n",
    "### http://scikit-learn.org/stable/modules/pipeline.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "\tAccuracy: 0.82733\tPrecision: 0.35804\tRecall: 0.37200\tF1: 0.36488\tF2: 0.36912\n",
      "\tTotal predictions: 15000\tTrue positives:  744\tFalse positives: 1334\tFalse negatives: 1256\tTrue negatives: 11666\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Decision Tree\n",
    "clf_tree = tree.DecisionTreeClassifier()\n",
    "clf_tree.fit(features_train, labels_train)\n",
    "tester.dump_classifier_and_data(clf_tree, my_dataset, features_list)\n",
    "tester.main();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree Feature Importances:\n",
      "\n",
      "shared_receipt_with_poi : 0.3591\n",
      "deferred_income : 0.2029\n",
      "deferral_payments : 0.1426\n",
      "total_payments : 0.1242\n",
      "expenses : 0.1119\n",
      "exercised_stock_options : 0.0594\n",
      "bonus : 0.0000\n",
      "long_term_incentive : 0.0000\n",
      "custom_ratio_to_poi : 0.0000\n",
      "custom_ratio_from_poi : 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Get the feature importances of the DecisionTree Classifier\n",
    "tree_feature_importances = (clf_tree.feature_importances_)\n",
    "tree_features = zip(tree_feature_importances, features_list[1:])\n",
    "tree_features = sorted(tree_features, key= lambda x:x[0], reverse=True)\n",
    "\n",
    "# Display the feature names and importance values\n",
    "print('Tree Feature Importances:\\n')\n",
    "for i in range(len(features_list) - 1):\n",
    "    print('{} : {:.4f}'.format(tree_features[i][1], tree_features[i][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
      "    n_clusters=2, n_init=10, n_jobs=1, precompute_distances='auto',\n",
      "    random_state=None, tol=0.0001, verbose=0)\n",
      "\tAccuracy: 0.81427\tPrecision: 0.20227\tRecall: 0.13350\tF1: 0.16084\tF2: 0.14324\n",
      "\tTotal predictions: 15000\tTrue positives:  267\tFalse positives: 1053\tFalse negatives: 1733\tTrue negatives: 11947\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Kmeans Clustering\n",
    "clf_km = KMeans(n_clusters=2)\n",
    "clf_km.fit(features_train, labels_train)\n",
    "tester.dump_classifier_and_data(clf_km, my_dataset, features_list)\n",
    "tester.main();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
